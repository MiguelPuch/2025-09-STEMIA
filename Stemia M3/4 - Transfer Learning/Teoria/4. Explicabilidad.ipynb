{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicabilidad\n",
    "\n",
    "Como vimos durante los ejercicios con distintos modelos de datos, algunos de ellos muestran una estructura interpretable donde podemos entender la base sobre la que ciertas decisiones han sido tomadas. \n",
    "\n",
    "En otros casos, esta comprensión no es tan sencilla, de forma que los modelos que no facilitan tanto la interpretabilidad son consideradas **cajas negras**. Dentro de esta categorización los modelos basados en árboles de decision son los llamados **cajas blancas** aunque el empleo de técnicas de ensembling puede hacer que se reduzca significativamente nuestra capacidad de comprensión.\n",
    "\n",
    "## Relevancia\n",
    "\n",
    "Nuestros modelos se fijarán en distinta características del conjunto de datos, pero ¿en cuales? Es una pregunta muy relevante y existen multitud de casos donde no habernos hecho esta pregunta a supuesto un problema.\n",
    "\n",
    "Tenemos casos donde el impacto de un modelo no auditado a tenido impactos penales: https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
    "\n",
    "Casos donde el impacto puede aumentar sesgos existentes en la sociedad\n",
    "\n",
    "![sesgos](../assets/images/sesgo.png)\n",
    "\n",
    "O casos donde podemos exponer a riesgos a los usuarios de nuestras aplicaciones o que sean sujetos de malas prácticas.\n",
    "\n",
    "* https://ieeexplore.ieee.org/document/8601309\n",
    "* https://arxiv.org/abs/1712.09665\n",
    "\n",
    "\n",
    "## Interpretar\n",
    "\n",
    "Esencialmente lo que queremos entender ante una decisión tomada por un modelo es en base a qué criterios ha sido esa decisión tomada. Ya que aunque estadísticamente tenga sentido, podría resultar que debido a sesgos de los datos nuestra decisión no sea justa o ética.\n",
    "\n",
    "### Frameworks\n",
    "\n",
    "Algunos frameworks populares en este dominio son:\n",
    "\n",
    "* Shap (https://shap.readthedocs.io/en/latest/index.html)\n",
    "* InterpretML (https://interpret.ml/)\n",
    "* Fairlearn (https://fairlearn.org/)\n",
    "\n",
    "#### Valores Shapley\n",
    "\n",
    "Los valores Shapley (SHAP) se basan en la teoría de juegos, específicamente en el concepto de valores de Shapley, desarrollado por Lloyd S. Shapley en 1953. Esta teoría se utiliza para asignar un valor de importancia a cada característica (o “jugador”) en un modelo, midiendo su contribución a la predicción global.\n",
    "\n",
    "![shap](https://christophm.github.io/interpretable-ml-book/images/shapley-instance-intervention.jpg)\n",
    "\n",
    "* **Coaliciones y agregación** SHAP crea coaliciones de características (conjuntos de jugadores sin repetición) y calcula la contribución de cada característica a la predicción mediante la agregación de valores Shapley. Esto permite evaluar la influencia individual de cada característica en la predicción, considerando su interacción con otras características.\n",
    "\n",
    "* **Linealidad y aditividad** Los valores SHAP se representan como un método de atribución de características aditiva y lineal. Esto significa que la contribución de cada característica se puede sumar para obtener la contribución total a la predicción. Esta propiedad lineal y aditiva facilita la interpretación de los resultados y la visualización de las explicaciones.\n",
    "\n",
    "* **Unidad atómica** Los valores SHAP se consideran la “unidad atómica” de las interpretaciones globales. Esto significa que los valores SHAP son la base para las explicaciones locales y globales, permitiendo una comprensión coherente y objetiva de cómo cada característica influye en la predicción.\n",
    "\n",
    "* **Transferible a modelos no lineales**\n",
    "Aunque la teoría de juegos se basa en suposiciones lineales, los valores SHAP se pueden aplicar a modelos no lineales, como modelos de árboles o redes neuronales, mediante aproximaciones y optimizaciones. Esto permite extender la explicabilidad a un amplio rango de modelos de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluso podemos usar estos valores para entender qué partes de una imagen influyeron en la decisión de una [red neuronal convolucional](https://shap.readthedocs.io/en/latest/example_notebooks/image_examples/image_classification/Explain%20ResNet50%20using%20the%20Partition%20explainer.html#Visualizing-SHAP-values-output).\n",
    "\n",
    "Es importante porque esto nos ayuda a entender las decisiones de nuestro modelo. Dado un modelo que muestre las siguientes métricas\n",
    "\n",
    "![perrolobo](../assets/images/perrolobo.png)\n",
    "\n",
    "qué decís que diría ante un nuevo caso como este?\n",
    "\n",
    "![corgi](../assets/images/corgi.png)\n",
    "\n",
    "La respuesta la tenéis aquí: https://arxiv.org/pdf/1602.04938 \n",
    "\n",
    "Tenéis mucho más detalle en la siguiente página: https://christophm.github.io/interpretable-ml-book"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspt2025-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
